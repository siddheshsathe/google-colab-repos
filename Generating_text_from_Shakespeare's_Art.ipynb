{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Generating text from Shakespeare's Art.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddheshsathe/google-colab-repos/blob/master/Generating_text_from_Shakespeare's_Art.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QcMbWOyEFo1",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Words with Shakespeare's Art"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VsHsOn1EFo9",
        "colab_type": "text"
      },
      "source": [
        "Listing the points that we'll be following to achieve the above task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k83VVRCEFpA",
        "colab_type": "text"
      },
      "source": [
        "1. Download data set and remove some data from the txt file which has mostly licensing and other info (This is done since I'm not distributing this to anybody, it's just for my own study purpose)\n",
        "3. Read lines from the file and store them all in a list\n",
        "4. Convert those all lines to lower case and without any punctuations. This will be our corpus\n",
        "5. Initialize the `Tokernizer()` instance and fit it over our corpus\n",
        "6. Above step will give us our total number of words \n",
        "7. Using the `tokernizer` instance, we'll convert all our corpus lines to `input_sequences` which will be a set of numbers and not the words; this is what a neural network expects\n",
        "8. Since the lengths of every line are different, we'll have to pad these input sequences with some numbers which ideally is 0. This gives us a rectangular `input_sequence`\n",
        "9. Now our input data for training is ready in the format what a machine expects\n",
        "10. We'll create a model using `LSTM`, `Dense`, `Embedding` layers. There's no predefined model structure, we'll use some random layer structures.\n",
        "11. Fit the model for certain number of epochs and will plot the graph of how training went in terms of losses and accuracy\n",
        "12. Now, it's time to generate the text\n",
        "13. As it's LSTM model, we'll need to give some seed text so that it'll generate the text based on that input\n",
        "14. Convert the input text (which obviously a system can't understand) to sequences as we did for training data\n",
        "15. Pad it for getting a shape of expected input shape\n",
        "16. Do a prediction on this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ukw1_WbEFpC",
        "colab_type": "text"
      },
      "source": [
        "## 1. Download dataset\n",
        "Download the data set from [Link](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt)\n",
        "<br>\n",
        "You may edit the file as I mentioned above in highlights. I've edited and renamed it to `shakespeare.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayyhFdSvEgS6",
        "colab_type": "code",
        "outputId": "86114ef8-f601-4b30-e3f0-3e7187c6b9c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "!wget https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-17 04:14:35--  https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
            "Resolving ocw.mit.edu (ocw.mit.edu)... 23.62.77.179, 2600:1409:a:39c::18a8, 2600:1409:a:39a::18a8\n",
            "Connecting to ocw.mit.edu (ocw.mit.edu)|23.62.77.179|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5458199 (5.2M) [text/plain]\n",
            "Saving to: ‘t8.shakespeare.txt’\n",
            "\n",
            "t8.shakespeare.txt  100%[===================>]   5.21M  4.96MB/s    in 1.1s    \n",
            "\n",
            "2020-02-17 04:14:37 (4.96 MB/s) - ‘t8.shakespeare.txt’ saved [5458199/5458199]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NZ8QUjzEFpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = 't8.shakespeare.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP4E-lzbEFpL",
        "colab_type": "text"
      },
      "source": [
        "## 2. Read the file\n",
        "Read the lines from file and store them in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QypWZtU7EFpO",
        "colab_type": "code",
        "outputId": "9ca69999-837a-4d77-bb92-b54dba06fd37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "complete_file = open(file_path, 'r').readlines()\n",
        "type(complete_file)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlzBU_R3EFpV",
        "colab_type": "text"
      },
      "source": [
        "## 3. Create corpus\n",
        "Convert the lines in to all lower case and remove any punctuations. <br>\n",
        "This is our corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdzMFHJ3EFpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "def remove_punc_and_lower(line):\n",
        "    line = [character.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "            for character in line if character not in string.punctuation]\n",
        "    return \"\".join(line)\n",
        "\n",
        "corpus = [remove_punc_and_lower(line) for line in complete_file]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mEI9RMuEFpe",
        "colab_type": "text"
      },
      "source": [
        "## 4. Initialize tokenizer\n",
        "This will help us convert our text based input data to a numbers based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXD1L3xHEFpf",
        "colab_type": "code",
        "outputId": "f395c23d-61fa-4970-b1e9-21de08a66054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "%tensorflow_version 2.0.1\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.0.1`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltGK-R29EFpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_corpus_length = 7000 \n",
        "# This is the length for fitting tokenizer and creating labels. \n",
        "# This is chosen as an experiment. If all corpus used, it gives memory error on my setup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KGzJemLEFpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgqpu17ZEFps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(corpus[:max_corpus_length])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvNVDS27EFpw",
        "colab_type": "code",
        "outputId": "2e14ae5d-f847-44ae-d00d-1745325eeb52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "total_words = len(tokenizer.word_index) + 1\n",
        "total_words"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6298"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUKYD4viEFp0",
        "colab_type": "text"
      },
      "source": [
        "## 5. Converting to `input_sequences`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu9nczaTEFp1",
        "colab_type": "code",
        "outputId": "364600f3-6cdd-4536-9bed-760be3c95629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "input_sequences = []\n",
        "for line in corpus[:max_corpus_length]:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        seq = token_list[:i+1]\n",
        "        input_sequences.append(seq)\n",
        "input_sequences[:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[23, 11],\n",
              " [23, 11, 1],\n",
              " [23, 11, 1, 2790],\n",
              " [23, 11, 1, 2790, 209],\n",
              " [23, 11, 1, 2790, 209, 868],\n",
              " [23, 11, 1, 2790, 209, 868, 1861],\n",
              " [23, 11, 1, 2790, 209, 868, 1861, 28],\n",
              " [23, 11, 1, 2790, 209, 868, 1861, 28, 222],\n",
              " [23, 11, 1, 2790, 209, 868, 1861, 28, 222, 264],\n",
              " [23, 11, 1, 2790, 209, 868, 1861, 28, 222, 264, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udtqwt2GEFp4",
        "colab_type": "text"
      },
      "source": [
        "## 6. Pad sequence\n",
        "We'll need to pad the sequences as the `input_sequences` are not in rectangular format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-JcwjOxEFp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FEEJYXkEFp9",
        "colab_type": "code",
        "outputId": "6fe49a4b-477a-452c-e92c-c7dcaecf500b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "longest_sequence = max([len(seq) for seq in input_sequences])\n",
        "print('Longest Seq: ', longest_sequence)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest Seq:  16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdInownbEFqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_sequences = pad_sequences(input_sequences, maxlen=longest_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIKhT630EFqG",
        "colab_type": "code",
        "outputId": "8a9f6b86-be45-481f-d0c7-f3903db95de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "input_sequences.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43401, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Fu9AMGEFqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = input_sequences[:, :-1]\n",
        "y_train = input_sequences[:, -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-x1f8LNEFqN",
        "colab_type": "text"
      },
      "source": [
        "## 7. Converting labels to one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiX7y0GiEFqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_ga7O3LEFqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = to_categorical(y_train, num_classes=total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0hCx1SgEFqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF2w5K6nEFqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijcZp5WgEFqf",
        "colab_type": "code",
        "outputId": "799e3365-e0d7-4c8f-c524-fe15bdc47fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(34720, 15) (8681, 15) (34720, 6298) (8681, 6298)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e-Ux8viEFqi",
        "colab_type": "text"
      },
      "source": [
        "## 8. Model creation\n",
        "Now, we've all required data in all proper formats for feeding in to the network.\n",
        "<br>\n",
        "`X_train`: Our input training data <br>\n",
        "`y_train`: Our input training labels <br>\n",
        "`longest_sequence`: longest sequence present in our input data. This is equal to the number of columns in `y_train` one-hot encoded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxZ4mS0DEFqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwSCobp-EFqm",
        "colab_type": "code",
        "outputId": "462c101b-3745-4100-b65c-8339cdf0ea66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(total_words, 10, input_length=longest_sequence-1))\n",
        "\n",
        "model.add(LSTM(512))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 15, 10)            62980     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 512)               1071104   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6298)              3230874   \n",
            "=================================================================\n",
            "Total params: 4,364,958\n",
            "Trainable params: 4,364,958\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHFIOfQQEFqp",
        "colab_type": "code",
        "outputId": "1097eb2a-c150-4566-a184-896ec85304c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 34720 samples, validate on 8681 samples\n",
            "Epoch 1/20\n",
            "34720/34720 [==============================] - 19s 544us/sample - loss: 6.8623 - accuracy: 0.0283 - val_loss: 6.7143 - val_accuracy: 0.0381\n",
            "Epoch 2/20\n",
            "34720/34720 [==============================] - 11s 325us/sample - loss: 6.5561 - accuracy: 0.0358 - val_loss: 6.6879 - val_accuracy: 0.0405\n",
            "Epoch 3/20\n",
            "34720/34720 [==============================] - 11s 321us/sample - loss: 6.4157 - accuracy: 0.0402 - val_loss: 6.6952 - val_accuracy: 0.0448\n",
            "Epoch 4/20\n",
            "34720/34720 [==============================] - 11s 321us/sample - loss: 6.2939 - accuracy: 0.0453 - val_loss: 6.7267 - val_accuracy: 0.0511\n",
            "Epoch 5/20\n",
            "34720/34720 [==============================] - 11s 324us/sample - loss: 6.1456 - accuracy: 0.0530 - val_loss: 6.7303 - val_accuracy: 0.0596\n",
            "Epoch 6/20\n",
            "34720/34720 [==============================] - 11s 325us/sample - loss: 5.9799 - accuracy: 0.0631 - val_loss: 6.7747 - val_accuracy: 0.0645\n",
            "Epoch 7/20\n",
            "34720/34720 [==============================] - 11s 325us/sample - loss: 5.7863 - accuracy: 0.0723 - val_loss: 6.8227 - val_accuracy: 0.0723\n",
            "Epoch 8/20\n",
            "34720/34720 [==============================] - 11s 325us/sample - loss: 5.5715 - accuracy: 0.0811 - val_loss: 6.9097 - val_accuracy: 0.0725\n",
            "Epoch 9/20\n",
            "34720/34720 [==============================] - 11s 323us/sample - loss: 5.3307 - accuracy: 0.0879 - val_loss: 7.0353 - val_accuracy: 0.0702\n",
            "Epoch 10/20\n",
            "34720/34720 [==============================] - 11s 309us/sample - loss: 5.0740 - accuracy: 0.1032 - val_loss: 7.1451 - val_accuracy: 0.0731\n",
            "Epoch 11/20\n",
            "34720/34720 [==============================] - 11s 316us/sample - loss: 4.8096 - accuracy: 0.1222 - val_loss: 7.2819 - val_accuracy: 0.0755\n",
            "Epoch 12/20\n",
            "34720/34720 [==============================] - 11s 324us/sample - loss: 4.5300 - accuracy: 0.1455 - val_loss: 7.4218 - val_accuracy: 0.0740\n",
            "Epoch 13/20\n",
            "34720/34720 [==============================] - 12s 332us/sample - loss: 4.2587 - accuracy: 0.1758 - val_loss: 7.5583 - val_accuracy: 0.0700\n",
            "Epoch 14/20\n",
            "34720/34720 [==============================] - 11s 322us/sample - loss: 4.0033 - accuracy: 0.2118 - val_loss: 7.7120 - val_accuracy: 0.0696\n",
            "Epoch 15/20\n",
            "34720/34720 [==============================] - 11s 323us/sample - loss: 3.7622 - accuracy: 0.2469 - val_loss: 7.8262 - val_accuracy: 0.0707\n",
            "Epoch 16/20\n",
            "34720/34720 [==============================] - 11s 324us/sample - loss: 3.5483 - accuracy: 0.2793 - val_loss: 7.9743 - val_accuracy: 0.0677\n",
            "Epoch 17/20\n",
            "34720/34720 [==============================] - 11s 322us/sample - loss: 3.3440 - accuracy: 0.3157 - val_loss: 8.0608 - val_accuracy: 0.0719\n",
            "Epoch 18/20\n",
            "34720/34720 [==============================] - 11s 323us/sample - loss: 3.1636 - accuracy: 0.3444 - val_loss: 8.1648 - val_accuracy: 0.0708\n",
            "Epoch 19/20\n",
            "34720/34720 [==============================] - 11s 326us/sample - loss: 3.0080 - accuracy: 0.3735 - val_loss: 8.2728 - val_accuracy: 0.0697\n",
            "Epoch 20/20\n",
            "34720/34720 [==============================] - 12s 341us/sample - loss: 2.8580 - accuracy: 0.4014 - val_loss: 8.3147 - val_accuracy: 0.0673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd275beb860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raIBW8s2EFqs",
        "colab_type": "text"
      },
      "source": [
        "## 9. Generate Text\n",
        "Now it's a time to generate the text from our trained model.\n",
        "<br>\n",
        "Here, we'll require to give a seed text for model to start predicting the next words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1xLF2VsEFqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(seed_word):\n",
        "    tokens = tokenizer.texts_to_sequences([seed_word])[0]\n",
        "    # Padding\n",
        "    tokens = pad_sequences([tokens], maxlen=longest_sequence-1)\n",
        "    # Prediction\n",
        "    pred = model.predict_classes(tokens)\n",
        "    return pred\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xJiUyYdEFqw",
        "colab_type": "text"
      },
      "source": [
        "Here, we've created a dict to map the word directly with the output prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6iycS9KEFqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordDict = {}\n",
        "for k, v in tokenizer.word_index.items():\n",
        "    wordDict[v] = k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu_aC8csEFq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ddb59567-52bc-4290-82a3-bf5f6363e09a"
      },
      "source": [
        "seed_word = \"Siddhesh\"\n",
        "for i in range(50):\n",
        "    word = wordDict[generate_text(seed_word)[0]]\n",
        "    seed_word += \" \" + word\n",
        "\n",
        "print(seed_word)\n",
        "    "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Siddhesh i have no precious pilgrim at all a fruitful wise and a great wise shakes in a thousand letters and a velvet as may be as as as a more and in the king a end of a mans as of a mans and of a mans as of a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvUYP4KQEFq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}